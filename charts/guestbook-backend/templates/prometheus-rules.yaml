{{- if .Values.monitoring.prometheusRules.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "guestbook-backend.fullname" . }}-rules
  labels:
    {{- include "guestbook-backend.labels" . | nindent 4 }}
    app.kubernetes.io/instance: {{ .Values.monitoring.prometheusOperator.instance | default "kube-prometheus-stack" }}
    app.kubernetes.io/part-of: {{ .Values.monitoring.prometheusOperator.name | default "kube-prometheus-stack" }}
    app: {{ .Values.monitoring.prometheusOperator.name | default "kube-prometheus-stack" }}
    release: {{ .Values.monitoring.prometheusOperator.release | default "kube-prometheus-stack" }}
  namespace: {{ .Values.monitoring.namespace }}
spec:
  groups:
  - name: guestbook-backend.rules
    rules:
    # Pod Health
    - alert: GuestbookBackendPodsNotReady
      expr: sum(kube_pod_status_ready{condition="true", namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*"}) < {{ .Values.replicaCount | default 1 }}
      for: {{ .Values.monitoring.alerting.podsNotReadyDuration | default "5m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.podsNotReadySeverity | default "critical" }}
      annotations:
        summary: "Not all backend pods are ready"
        description: "Some backend pods have been in non-ready state for more than {{ .Values.monitoring.alerting.podsNotReadyDuration | default "5m" }}"

    # Pod Restarts
    - alert: GuestbookBackendFrequentRestarts
      expr: increase(kube_pod_container_status_restarts_total{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*"}[1h]) > {{ .Values.monitoring.alerting.restartThreshold | default 2 }}
      for: {{ .Values.monitoring.alerting.restartDuration | default "5m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.restartSeverity | default "warning" }}
      annotations:
        summary: "Frequent backend pod restarts detected"
        description: "Backend pod has restarted more than {{ .Values.monitoring.alerting.restartThreshold | default 2 }} times in the last hour"

    # CPU Usage
    - alert: GuestbookBackendHighCPUUsage
      expr: |
        sum(rate(container_cpu_usage_seconds_total{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", container!=""}[5m]))
        /
        sum(kube_pod_container_resource_limits{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", resource="cpu"}) > {{ .Values.monitoring.alerting.cpuThreshold | default 0.85 }}
      for: {{ .Values.monitoring.alerting.cpuDuration | default "15m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.cpuSeverity | default "warning" }}
      annotations:
        summary: "High backend CPU usage"
        description: "Backend CPU usage is above {{ mul (.Values.monitoring.alerting.cpuThreshold | default 0.85) 100 }}% of the limit for {{ .Values.monitoring.alerting.cpuDuration | default "15m" }}"

    # Memory Usage
    - alert: GuestbookBackendHighMemoryUsage
      expr: |
        sum(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", container!=""})
        /
        sum(kube_pod_container_resource_limits{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", resource="memory"}) > {{ .Values.monitoring.alerting.memoryThreshold | default 0.85 }}
      for: {{ .Values.monitoring.alerting.memoryDuration | default "15m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.memorySeverity | default "warning" }}
      annotations:
        summary: "High backend memory usage"
        description: "Backend memory usage is above {{ mul (.Values.monitoring.alerting.memoryThreshold | default 0.85) 100 }}% of the limit for {{ .Values.monitoring.alerting.memoryDuration | default "15m" }}"

    # OOM Kills
    - alert: GuestbookBackendOOMKilled
      expr: kube_pod_container_status_last_terminated_reason{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", reason="OOMKilled"} == 1
      for: {{ .Values.monitoring.alerting.oomDuration | default "1m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.oomSeverity | default "critical" }}
      annotations:
        summary: "Backend container OOM killed"
        description: "Backend container was terminated due to Out Of Memory condition"

    # Pod Pending Status
    - alert: GuestbookBackendPodsPending
      expr: kube_pod_status_phase{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", phase="Pending"} == 1
      for: {{ .Values.monitoring.alerting.pendingDuration | default "5m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.pendingSeverity | default "warning" }}
      annotations:
        summary: "Backend pods in pending state"
        description: "Some backend pods are stuck in pending state for more than {{ .Values.monitoring.alerting.pendingDuration | default "5m" }}"

    # Container Status
    - alert: GuestbookBackendContainerWaiting
      expr: kube_pod_container_status_waiting{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*"} == 1
      for: {{ .Values.monitoring.alerting.containerWaitingDuration | default "5m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.containerWaitingSeverity | default "warning" }}
      annotations:
        summary: "Backend container in waiting state"
        description: "Backend container has been in waiting state for more than {{ .Values.monitoring.alerting.containerWaitingDuration | default "5m" }}"

    # Network Errors
    - alert: GuestbookBackendHighNetworkErrors
      expr: |
        sum(rate(container_network_errors_total{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*"}[5m])) > {{ .Values.monitoring.alerting.networkErrorThreshold | default 0.1 }}
      for: {{ .Values.monitoring.alerting.networkErrorDuration | default "15m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.networkErrorSeverity | default "warning" }}
      annotations:
        summary: "High network error rate detected"
        description: "Backend is experiencing elevated network errors for {{ .Values.monitoring.alerting.networkErrorDuration | default "15m" }}"

    # No Pods Running
    - alert: GuestbookBackendNoPods
      expr: kube_deployment_status_replicas_available{namespace="{{ .Release.Namespace }}", deployment="guestbook-backend"} == 0
      for: {{ .Values.monitoring.alerting.noPodsRunningDuration | default "1m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.noPodsRunningSeverity | default "critical" }}
      annotations:
        summary: "No backend pods are running"
        description: "There are no available pods running for the backend deployment"

    # Database Connection Issues
    - alert: GuestbookBackendDatabaseConnectionFailures
      expr: |
        increase(flask_http_request_total{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", status="503"}[5m]) > 0
      for: {{ .Values.monitoring.alerting.databaseConnectionDuration | default "2m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.databaseConnectionSeverity | default "critical" }}
      annotations:
        summary: "Database connection issues detected"
        description: "Backend is returning 503 errors which might indicate database connection problems"

    # HTTP Request Latency
    - alert: GuestbookBackendHighLatency
      expr: |
        histogram_quantile(0.95, rate(flask_http_request_duration_seconds_bucket{namespace="{{ .Release.Namespace }}", service="guestbook-backend"}[5m])) > {{ .Values.monitoring.alerting.latencyThreshold | default 2 }}
      for: {{ .Values.monitoring.alerting.latencyDuration | default "10m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.latencySeverity | default "warning" }}
      annotations:
        summary: "High HTTP request latency"
        description: "95th percentile of HTTP request latency is above {{ .Values.monitoring.alerting.latencyThreshold | default 2 }} seconds"

    # HTTP Error Rate
    - alert: GuestbookBackendHighErrorRate
      expr: |
        sum(rate(flask_http_request_total{namespace="{{ .Release.Namespace }}", service="guestbook-backend", status=~"5.."}[5m]))
        /
        sum(rate(flask_http_request_total{namespace="{{ .Release.Namespace }}", service="guestbook-backend"}[5m])) > {{ .Values.monitoring.alerting.errorRateThreshold | default 0.05 }}
      for: {{ .Values.monitoring.alerting.errorRateDuration | default "5m" }}
      labels:
        severity: {{ .Values.monitoring.alerting.errorRateSeverity | default "critical" }}
      annotations:
        summary: "High HTTP error rate"
        description: "More than {{ mul (.Values.monitoring.alerting.errorRateThreshold | default 0.05) 100 }}% of requests are resulting in 5xx errors"

    # Recording Rules for metrics
    - record: guestbook_backend:cpu_usage_percentage
      expr: |
        100 * sum(rate(container_cpu_usage_seconds_total{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", container!=""}[5m]))
        /
        sum(kube_pod_container_resource_limits{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", resource="cpu"})

    - record: guestbook_backend:memory_usage_percentage
      expr: |
        100 * sum(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", container!=""})
        /
        sum(kube_pod_container_resource_limits{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", resource="memory"})

    - record: guestbook_backend:network_error_rate
      expr: |
        sum(rate(container_network_errors_total{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*"}[5m]))

    - record: guestbook_backend:request_latency_p95
      expr: |
        histogram_quantile(0.95, rate(flask_http_request_duration_seconds_bucket{namespace="{{ .Release.Namespace }}", service="guestbook-backend"}[5m]))

    - record: guestbook_backend:error_rate_percentage
      expr: |
        100 * sum(rate(flask_http_request_total{namespace="{{ .Release.Namespace }}", service="guestbook-backend", status=~"5.."}[5m]))
        /
        sum(rate(flask_http_request_total{namespace="{{ .Release.Namespace }}", service="guestbook-backend"}[5m]))

    - record: guestbook_backend:database_error_rate
      expr: |
        sum(rate(flask_http_request_total{namespace="{{ .Release.Namespace }}", pod=~"guestbook-backend.*", status="503"}[5m]))
{{- end }}
